{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stpwords = stopwords.words('english')\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all data and test ids\n",
    "all_data = pd.read_csv('alldata.tsv', sep='\\t')\n",
    "testIDs = pd.read_csv('project3_splits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cleaning the text\n",
    "def clean_corpus(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - string\n",
    "    OUTPUT\n",
    "    clean text\n",
    "    This function processes the input using the following steps :\n",
    "    1. Remove punctuation characters\n",
    "    2. Remove stop words\n",
    "    '''\n",
    "    # Remove punctuation characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "        \n",
    "    clean_text = ''\n",
    "    for word in tokens:\n",
    "        clean_tok = word.lower().strip()\n",
    "        if clean_tok not in stpwords:\n",
    "            clean_text += f'{clean_tok} '\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the cleaning column on the dataset\n",
    "all_data['clean_text'] = all_data['review'].apply(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating test and train splits\n",
    "# for j in range(5):\n",
    "#     dir_str = \"split_\"\n",
    "#     os.mkdir(dir_str+str(j+1))\n",
    "    \n",
    "#     train = all_data.loc[~all_data['id'].isin(list(testIDs.iloc[:,j])), [\"id\",\"sentiment\",\"review\"]]\n",
    "#     test = all_data.loc[all_data['id'].isin(list(testIDs.iloc[:,j])), [\"id\",\"review\"]]\n",
    "#     test_y = all_data.loc[all_data['id'].isin(list(testIDs.iloc[:,j])), [\"id\",\"sentiment\",\"score\"]]\n",
    "    \n",
    "#     tmp_file_name1 = \"split_\" + str(j+1) +\"/\" + \"train.csv\"\n",
    "#     train.to_csv(tmp_file_name1, index = False)\n",
    "    \n",
    "#     tmp_file_name2 = \"split_\" + str(j+1) +\"/\" + \"test.csv\"\n",
    "#     test.to_csv(tmp_file_name2, index = False)\n",
    "    \n",
    "#     tmp_file_name3 = \"split_\" + str(j+1) +\"/\" + \"test_y.csv\"\n",
    "#     test_y.to_csv(tmp_file_name3, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=3, ngram_range=(1, 2), smooth_idf=1,\n",
       "                stop_words='english', strip_accents='unicode', sublinear_tf=1,\n",
       "                token_pattern='\\\\w{1,}', use_idf=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the Tfidf vectorizer\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "tfv.fit(list(all_data.review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  :  0.8964805560894356\n",
      "2  :  0.892965452098464\n",
      "3  :  0.8950435582293231\n",
      "4  :  0.8955396603688884\n",
      "5  :  0.8950965361244289\n"
     ]
    }
   ],
   "source": [
    "# Training a logistic model\n",
    "for i in range(5):\n",
    "    file_nm1 = 'split_'+str(i+1)+'/train.tsv'\n",
    "    train = pd.read_csv(file_nm1, sep='\\t')\n",
    "    \n",
    "    del train['id']\n",
    "    train_y = train.sentiment\n",
    "    xtrain = train.review\n",
    "    \n",
    "    file_nm2 = 'split_'+str(i+1)+'/test.tsv'\n",
    "    test = pd.read_csv(file_nm2, sep='\\t')\n",
    "    del test['id']\n",
    "    \n",
    "    file_nm3 = 'split_'+str(i+1)+'/test_y.tsv'\n",
    "    test_y = pd.read_csv(file_nm3, sep='\\t')\n",
    "    xtest = test.review\n",
    "    \n",
    "    # creating label encoder\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    y = lbl_enc.fit_transform(train.sentiment)\n",
    "\n",
    "    xtrain_tfv =  tfv.transform(xtrain) \n",
    "    xtest_tfv = tfv.transform(xtest)\n",
    "\n",
    "    clf = LogisticRegression(C=1.0)\n",
    "    clf.fit(xtrain_tfv, y)\n",
    "    predictions = clf.predict(xtest_tfv)\n",
    "\n",
    "    print(str(i+1),\" : \",roc_auc_score(predictions,test_y.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:53:08] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "1  :  0.8597650975544413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:54:36] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "2  :  0.8586560262223772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:56:03] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3  :  0.8631979561874471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:57:30] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "4  :  0.8593713367427287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:58:57] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "5  :  0.8606716078829169\n"
     ]
    }
   ],
   "source": [
    "# Training an xgboost model\n",
    "for i in range(5):\n",
    "    file_nm1 = 'split_'+str(i+1)+'/train.csv'\n",
    "    train = pd.read_csv(file_nm1)\n",
    "    \n",
    "    del train['id']\n",
    "    train_y = train.sentiment\n",
    "    xtrain = train.review\n",
    "    \n",
    "    file_nm2 = 'split_'+str(i+1)+'/test.csv'\n",
    "    test = pd.read_csv(file_nm2)\n",
    "    del test['id']\n",
    "    \n",
    "    file_nm3 = 'split_'+str(i+1)+'/test_y.csv'\n",
    "    test_y = pd.read_csv(file_nm3)\n",
    "    xtest = test.review\n",
    "    \n",
    "    # creating label encoder\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    y = lbl_enc.fit_transform(train.sentiment)\n",
    "\n",
    "    xtrain_tfv =  tfv.transform(xtrain) \n",
    "    xtest_tfv = tfv.transform(xtest)\n",
    "\n",
    "    clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "    \n",
    "    clf.fit(xtrain_tfv.tocsc(), y)\n",
    "    predictions = clf.predict(xtest_tfv.tocsc())\n",
    "\n",
    "    print(str(i+1),\" : \",roc_auc_score(predictions,test_y.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising rated highly older viewers younger ones however craftsmanship completeness film anyone enjoy pace steady constant characters full engaging relationships interactions natural showing need floods tears show emotion screams show fear shouting show dispute violence show anger naturally joyce short story lends film ready made structure perfect polished diamond small changes huston makes inclusion poem fit neatly truly masterpiece tact subtlety overwhelming beauty '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus(all_data['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_review = [clean_corpus(x) for x in all_data.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'naturally film main themes mortality nostalgia loss innocence perhaps surprising rated highly older viewers younger ones however craftsmanship completeness film anyone enjoy pace steady constant characters full engaging relationships interactions natural showing need floods tears show emotion screams show fear shouting show dispute violence show anger naturally joyce short story lends film ready made structure perfect polished diamond small changes huston makes inclusion poem fit neatly truly masterpiece tact subtlety overwhelming beauty '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['cleaned_review'] = cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [nltk.word_tokenize(x) for x in cleaned_review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(all_words, min_count=5, max_final_vocab=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x19daed34dc0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.58842558e-01, -8.44656348e-01, -4.87949133e-01, -9.40350354e-01,\n",
       "        2.66864657e-01,  2.94531137e-02, -1.73610330e+00, -5.67825079e-01,\n",
       "        8.08339417e-01, -1.69808459e+00,  1.03057063e+00,  9.62835923e-03,\n",
       "       -2.43424967e-01, -4.38597918e-01, -6.84791803e-01,  1.76150572e+00,\n",
       "        8.65396798e-01,  1.86191440e+00, -1.70171249e+00,  2.79220670e-01,\n",
       "       -3.38357627e-01, -2.32526157e-02,  2.17913091e-01, -4.84722517e-02,\n",
       "        1.33072913e+00, -9.22531426e-01,  9.30391371e-01,  1.24785292e+00,\n",
       "        2.34947711e-01,  1.45575678e+00,  6.19320333e-01, -7.44338453e-01,\n",
       "       -2.53127009e-01, -1.18179643e+00, -2.42393002e-01,  4.00296271e-01,\n",
       "        1.19838119e+00,  7.86305845e-01,  1.48067081e+00,  2.49931979e+00,\n",
       "        1.02869177e+00, -1.41337092e-04, -1.42204297e+00, -5.92005849e-01,\n",
       "       -1.19590126e-01,  3.18989754e-01,  1.60031188e+00, -2.38928959e-01,\n",
       "        4.66436893e-01,  4.89191979e-01,  7.68740416e-01,  6.45394504e-01,\n",
       "        1.87487364e-01,  4.32365686e-01, -5.53805888e-01,  7.96083748e-01,\n",
       "        1.11906993e+00,  6.08766496e-01,  3.10447663e-01,  1.37458670e+00,\n",
       "       -1.69853902e+00, -8.70480537e-01, -5.69730960e-02, -1.19265854e+00,\n",
       "        8.82604361e-01,  9.96111333e-01,  8.45334649e-01, -4.08787392e-02,\n",
       "        2.84919068e-02,  1.55482590e-01,  6.49539709e-01,  2.55193245e-02,\n",
       "        7.67680585e-01,  1.06132376e+00,  1.09648442e+00, -3.56962860e-01,\n",
       "        9.48056757e-01,  4.28080738e-01,  1.32337320e+00, -8.66624832e-01,\n",
       "        9.72862840e-02, -7.80416191e-01, -8.69413987e-02, -6.65369153e-01,\n",
       "       -1.04461575e+00, -3.95922452e-01,  2.06669793e-02, -1.64902902e+00,\n",
       "       -1.75101042e-01, -6.57643676e-01,  8.06280851e-01, -2.99229771e-01,\n",
       "        9.91893634e-02, -2.08957195e+00, -8.72374415e-01, -6.12708151e-01,\n",
       "       -4.79826927e-01,  9.06530380e-01, -1.17541742e+00, -7.02109516e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = word2vec.wv['movie']\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
