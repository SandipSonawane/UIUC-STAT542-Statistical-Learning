{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cc000d-decd-4028-99ee-14cb7750a315",
   "metadata": {},
   "source": [
    "#### Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3cff05c-1bba-450d-be02-3102b8ff2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d74edaf7-08a3-4dfd-9eaa-2e7616d3be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'F21_proj2_data')\n",
    "train_ini = pd.read_csv(f'{data_path}/train_ini.csv',parse_dates = ['Date'], dayfirst = True)\n",
    "test_ini = pd.read_csv(f'{data_path}/fold_1.csv',parse_dates = ['Date'], dayfirst = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb03d58b-6e6a-481f-a9eb-a9220a1428ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  Dept       Date  Weekly_Sales  IsHoliday\n",
      "0      1     1 2010-02-05      24924.50      False\n",
      "1      1     1 2010-02-12      46039.49       True\n",
      "2      1     1 2010-02-19      41595.55      False\n",
      "3      1     1 2010-02-26      19403.54      False\n",
      "4      1     1 2010-03-05      21827.90      False\n",
      "5      1     1 2010-03-12      21043.39      False\n",
      "6      1     1 2010-03-19      22136.64      False\n",
      "7      1     1 2010-03-26      26229.21      False\n",
      "8      1     1 2010-04-02      57258.43      False\n",
      "9      1     1 2010-04-09      42960.91      False\n",
      "Shape of the training data:  (164115, 5)\n",
      "Shape of the training data:  (26559, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train_ini.head(10))\n",
    "print(\"Shape of the training data: \", train_ini.shape)\n",
    "print(\"Shape of the training data: \", test_ini.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "295e8bfe-c4d6-4f2b-b80d-e2120b32b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation functions\n",
    "train = train_ini.copy()\n",
    "test = test_ini.copy()\n",
    "train['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x else 0)\n",
    "test['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "\n",
    "# There are some negative values in the Weekly_Sales column, converting them to positive as they seem like data entry mistakes\n",
    "train['Weekly_Sales'] = abs(train['Weekly_Sales'])\n",
    "test['Weekly_Sales'] = abs(test['Weekly_Sales'])\n",
    "\n",
    "# extracting year and week from the date field\n",
    "train['Year'] = train['Date'].dt.year\n",
    "train['Week'] = train['Date'].dt.isocalendar().week\n",
    "\n",
    "test['Year'] = test['Date'].dt.year\n",
    "test['Week'] = test['Date'].dt.isocalendar().week\n",
    "\n",
    "# Sorting the data by Store, Department and Date\n",
    "train.sort_values(['Store','Dept','Date'], ignore_index=True, ascending=True, inplace=True)\n",
    "test.sort_values(['Store','Dept','Date'], ignore_index=True, ascending=True, inplace=True)\n",
    "\n",
    "\n",
    "# Creating a column which has the previous week sales as a separate column\n",
    "train['Last_Week_Sales'] = train.groupby(['Store','Dept'])['Weekly_Sales'].shift(1)\n",
    "test['Last_Week_Sales'] = test.groupby(['Store','Dept'])['Weekly_Sales'].shift(1)\n",
    "\n",
    "# Dropping date column as that information is already captured by Year and Week Column\n",
    "del train['Date']\n",
    "del test['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c4f1769-16aa-42c0-9ba9-93b705074eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 4th order difference for the weekly sale column\n",
    "col = 'Weekly_Sales'\n",
    "for order in [2,3,4]:\n",
    "    train[f'{col}_D{order}'] = train.groupby(['Store','Dept'])[col].diff(periods=order)\n",
    "    test[f'{col}_D{order}'] = test.groupby(['Store','Dept'])[col].diff(periods=order)\n",
    "    \n",
    "# Removing rows with missing values\n",
    "train = train.dropna(subset = list(train.columns))\n",
    "test = test.dropna(subset = list(test.columns))\n",
    "\n",
    "# Creating X_train, Y_train\n",
    "X_train = train.loc[:,~train.columns.isin(['Weekly_Sales'])]\n",
    "y_train = train.loc[:,train.columns.isin(['Weekly_Sales'])]\n",
    "\n",
    "# Creating X_test, Y_test\n",
    "X_test = test.loc[:,~test.columns.isin(['Weekly_Sales'])]\n",
    "y_test = test.loc[:,test.columns.isin(['Weekly_Sales'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb82980-9b2c-449d-815d-1008e7ca287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train_trans = scaler.transform(X_train)\n",
    "# X_test_trans = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db678c4a-846e-4705-8dde-a16ab10fa78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anurag\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:880: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestRegressor(n_estimators=500, n_jobs=-1,\n",
       "                                             oob_score=True, random_state=10),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [20, 25, 30, 35],\n",
       "                         'max_features': ['auto'],\n",
       "                         'min_samples_leaf': [15, 20, 25, 50]},\n",
       "             scoring='neg_mean_squared_error', verbose=10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting GBR, RF and NN on the training data set\n",
    "parameters = {'max_features':['auto'],'max_depth':[20,25,30,35],'min_samples_leaf':[15,20,25,50]}\n",
    "rf_mod = RandomForestRegressor(n_estimators=500, criterion='mse', oob_score=True, n_jobs=-1, random_state=10)\n",
    "grid_search = GridSearchCV(rf_mod, parameters, cv=5, n_jobs=-1, scoring='neg_mean_squared_error', verbose = 10)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6230b845-1770-45e2-a73e-68024d68b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_mod = grid_search.best_estimator_\n",
    "pred = fin_mod.predict(X_test).reshape(len(y_test), 1)\n",
    "weights = test['IsHoliday'].apply(lambda is_holiday:5 if is_holiday == 1 else 1).to_numpy().reshape(len(pred),1)\n",
    "actuals = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "851d714b-2666-4ff3-b241-c70fb028657e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1238.0560409568982"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(weights * np.abs(actuals - pred).to_numpy()) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc35a3d6-23dc-460c-ad0a-c1bf3e6adb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226.3597226409154"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.sum(np.abs(actuals - pred).to_numpy())) / len(np.abs(actuals - pred).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b9442e-66b3-40e7-aece-1565103c5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the data for a particular Store and Department id to csv\n",
    "# train.loc[((train['Store'] == 1) & (train['Dept'] == 1)) | ((train['Store'] == 2) & (train['Dept'] == 1))].to_csv('data_check.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625416e6-0a44-4e18-a556-d25196f4d313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352046a3-31ee-4e83-8f97-e89bd8462e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d903e7-4985-4043-8d56-180bdbf35a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
